{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63694543-37c0-4012-a001-15f5f8ad692b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "df = pd.read_excel(\"./Input.xlsx\", \"Sheet1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cc2f086-1eac-4b6f-94e8-6fe9007f8f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_404 = []\n",
    "def get_article(df):\n",
    "\n",
    "    \n",
    "    if os.path.exists(\"Articles\"):\n",
    "        shutil.rmtree(\"Articles\")\n",
    "    \n",
    "    os.makedirs(\"Articles\")\n",
    "\n",
    "    \n",
    "    for i in df.values:\n",
    "\n",
    "        id = i[0]\n",
    "        response = requests.get(i[1])\n",
    "        \n",
    "        if response.status_code == 404:\n",
    "            error_404.append(i[0])\n",
    "            continue\n",
    "            \n",
    "        soup = bs(response.text)\n",
    "\n",
    "        article_heading = soup.find(\"h1\", \"entry-title\")\n",
    "        if article_heading == None:\n",
    "            article_heading = soup.find(\"h1\", \"tdb-title-text\")\n",
    "\n",
    "        # article_div = soup.find(\"div\", \"td-post-content tagdiv-type\")\n",
    "        article_div = soup.find_all(\"div\", \"td-post-content tagdiv-type\")\n",
    "        flag = 0\n",
    "        \n",
    "        if article_div == []:\n",
    "            flag = 1\n",
    "            # article_div = soup.find(\"div\", \"tdb-block-inner td-fix-index\")\n",
    "            article_div = soup.find_all(\"div\", \"tdb-block-inner td-fix-index\")\n",
    "        # print(i)\n",
    "        \n",
    "        heading = article_heading.get_text(strip = False)\n",
    "        lb = \"\\n\"*2\n",
    "        if flag == 0:\n",
    "            article_text = article_div[0].get_text(strip = False)\n",
    "            # article_text = article_div[0].get_text(strip = True)\n",
    "            article_text = re.sub(r'\\n{2,}', '\\n\\n', article_text)\n",
    "        else:\n",
    "            article_text = article_div[14].get_text(strip = False)\n",
    "            # article_text = article_div[14].get_text(strip = True)\n",
    "            article_text = re.sub(r'\\n{2,}', '\\n\\n', article_text)\n",
    "        \n",
    "        article_text = article_text.strip()\n",
    "        last_line = article_text.rfind(\"\\n\")\n",
    "        article_text = article_text[:last_line]\n",
    "       \n",
    "        article = heading + lb + article_text\n",
    "        article = article.strip()\n",
    "\n",
    "        with open(\"./Articles/{}.txt\".format(id), \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(article)\n",
    "\n",
    "\n",
    "get_article(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1a36e64-597c-43e8-b92e-c4197b9144ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_stopwords(path):\n",
    "    \n",
    "    h0, h1, h2 = [], [], []\n",
    "    \n",
    "    for i in os.listdir(path):\n",
    "        file = path+\"/\"+i\n",
    "        \n",
    "        with open(file, \"r\") as f:\n",
    "            # print(f.read())\n",
    "            for i in f.readlines():\n",
    "                h0.append(i.strip())\n",
    "\n",
    "    for i in h0:\n",
    "        h1.append(i.split(\"|\"))\n",
    "\n",
    "    for i in h1:\n",
    "        for j in range(len(i)):\n",
    "            i[j] = i[j].strip()\n",
    "            h2.append(i[j])\n",
    "            \n",
    "    return h2\n",
    "\n",
    "def list_pn(path):\n",
    "    h1 = []\n",
    "    h2 = []\n",
    "    count = 0\n",
    "    for i in os.listdir(path):\n",
    "        file = path+\"/\"+i\n",
    "        if i == \"negative-words.txt\":\n",
    "            with open(file, \"r\") as f:\n",
    "                for i in f.readlines():\n",
    "                    h1.append(i.strip())\n",
    "                    \n",
    "        if i == \"positive-words.txt\":\n",
    "            with open(file, \"r\") as f:\n",
    "                for i in f.readlines():\n",
    "                    h2.append(i.strip())\n",
    "\n",
    "    return h1, h2\n",
    "\n",
    "\n",
    "stopwords = list_stopwords(\"./StopWords\")\n",
    "nwords, pwords = list_pn(\"./MasterDictionary\")\n",
    "\n",
    "def cleaning_np(stopwords):\n",
    "    for i in nwords:\n",
    "        if i in stopwords:\n",
    "            nwords.remove(i)\n",
    "            \n",
    "    for i in pwords:\n",
    "        if i in stopwords:\n",
    "            pwords.remove(i)\n",
    "\n",
    "cleaning_np(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd6bb9c8-7dca-46dd-9018-6699d576da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(path):\n",
    "\n",
    "    scores = []\n",
    "    pronouncing_dict = nltk.corpus.cmudict.dict()\n",
    "    stopwords_nltk = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    for i in os.listdir(path):\n",
    "        file = path + \"/\" + i\n",
    "        with open(file, \"r\", encoding = \"utf8\") as f:\n",
    "            # print(f.read())\n",
    "            article_t = f.read()\n",
    "                \n",
    "        article_token = nltk.tokenize.word_tokenize(article_t)\n",
    "        article_token_uncleaned = nltk.tokenize.word_tokenize(article_t)\n",
    "        article_sentences = nltk.sent_tokenize(article_t)\n",
    "        article_token_nltk_cleaned = nltk.tokenize.word_tokenize(article_t)\n",
    "        \n",
    "        for x in article_token:\n",
    "            if x in string.punctuation:\n",
    "                article_token.remove(x)\n",
    "                \n",
    "        for x1 in article_token:\n",
    "            if x1 in stopwords:\n",
    "                article_token.remove(x1)\n",
    "    \n",
    "        \n",
    "        for x in article_token_nltk_cleaned:\n",
    "            if x in string.punctuation:\n",
    "                article_token_nltk_cleaned.remove(x)\n",
    "                \n",
    "        for x1 in article_token_nltk_cleaned:\n",
    "            if x1 in stopwords_nltk:\n",
    "                article_token_nltk_cleaned.remove(x1)\n",
    "    \n",
    "        helper = []\n",
    "        \n",
    "        total_words = len(article_token)\n",
    "        total_words_nltk_cleaned = len(article_token_nltk_cleaned)\n",
    "        total_words_uncleaned = len(article_token_uncleaned)\n",
    "        total_sentences = len(article_sentences)\n",
    "        personal_pronouns = [ \"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"you\", \"they\", \"me\", \"you\", \"him\", \"her\", \"it\", \"us\", \"you\", \"them\", \"my\", \"mine\",\n",
    "        \"your\", \"yours\", \"his\", \"hers\", \"its\", \"our\", \"ours\", \"your\", \"yours\", \"their\", \"theirs\", \"myself\", \"yourself\", \"himself\", \"herself\",\n",
    "        \"itself\", \"ourselves\", \"yourselves\", \"themselves\"]\n",
    "        \n",
    "        \n",
    "        complex_words = []\n",
    "        syl_per_word = 0\n",
    "        for x4 in article_token_uncleaned:\n",
    "            if x4.lower() in pronouncing_dict:\n",
    "                verbal = pronouncing_dict[x4.lower()]\n",
    "                for p in verbal:\n",
    "                    syl_count = 0\n",
    "                    for x5 in p:\n",
    "                        if x5[-1].isdigit():\n",
    "                            syl_count += 1\n",
    "                            syl_per_word += syl_count\n",
    "                    if syl_count > 2:\n",
    "                        complex_words.append(x4)\n",
    "                    syl_per_word += syl_count\n",
    "        \n",
    "    \n",
    "        # calculating scores for all entity\n",
    "        \n",
    "        p_score = 0\n",
    "        n_score = 0\n",
    "        \n",
    "        for x3 in article_token:\n",
    "            if x3 in nwords:\n",
    "                n_score += 1\n",
    "            if x3 in pwords:\n",
    "                p_score += 1\n",
    "    \n",
    "        helper.append(p_score)\n",
    "        helper.append(n_score)\n",
    "    \n",
    "        \n",
    "        pol_score = (p_score - n_score) / ((p_score + n_score) + 0.000001)\n",
    "        helper.append(pol_score)\n",
    "        \n",
    "        sub_score = (p_score + n_score) / ((total_words) + 0.000001)\n",
    "        helper.append(sub_score)\n",
    "    \n",
    "        avg_sentence_length = total_words / total_sentences\n",
    "        helper.append(avg_sentence_length)\n",
    "    \n",
    "        complex_percentage = (len(complex_words) / total_words) * 100\n",
    "        helper.append(complex_percentage)\n",
    "    \n",
    "        fog_index = 0.4 * (avg_sentence_length + complex_percentage)\n",
    "        helper.append(fog_index)\n",
    "    \n",
    "        avg_words_per_sentence = total_words_uncleaned / total_sentences\n",
    "        helper.append(avg_words_per_sentence)\n",
    "    \n",
    "        complex_words_count = len(complex_words)\n",
    "        helper.append(complex_words_count)\n",
    "        \n",
    "        total_words_final = total_words_nltk_cleaned\n",
    "        helper.append(total_words_final)\n",
    "        \n",
    "        avg_syllable_each_word =  total_words / syl_per_word\n",
    "        helper.append(avg_syllable_each_word)\n",
    "        \n",
    "        total_personal_pronoun = 0\n",
    "        for x6 in article_token_uncleaned:\n",
    "            if  x6 == \"US\":\n",
    "                continue\n",
    "            elif x6.lower in personal_pronouns:\n",
    "                total_personal_pronoun += 1\n",
    "        helper.append(total_personal_pronoun)\n",
    "    \n",
    "        sum_char = 0\n",
    "        for x8 in article_token:\n",
    "            for j in x8:\n",
    "                sum_char += 1\n",
    "        avg_word_length = sum_char / total_words\n",
    "        helper.append(avg_word_length)\n",
    "    \n",
    "        scores.append(helper)\n",
    "        \n",
    "    return scores\n",
    "    \n",
    "analysis_score = get_scores(\"./Articles\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "840dbdd2-d8f8-4966-a00a-2586f56c867b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.046931</td>\n",
       "      <td>10.653846</td>\n",
       "      <td>23.345367</td>\n",
       "      <td>13.599685</td>\n",
       "      <td>17.666667</td>\n",
       "      <td>194.0</td>\n",
       "      <td>878.0</td>\n",
       "      <td>0.122911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.356197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.303371</td>\n",
       "      <td>0.087771</td>\n",
       "      <td>12.835443</td>\n",
       "      <td>46.646943</td>\n",
       "      <td>23.792954</td>\n",
       "      <td>21.493671</td>\n",
       "      <td>473.0</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>0.095202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.458580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>-0.327103</td>\n",
       "      <td>0.144205</td>\n",
       "      <td>14.549020</td>\n",
       "      <td>52.291105</td>\n",
       "      <td>26.736050</td>\n",
       "      <td>23.745098</td>\n",
       "      <td>388.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>0.093251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.195418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>11.897436</td>\n",
       "      <td>39.439655</td>\n",
       "      <td>20.534836</td>\n",
       "      <td>19.589744</td>\n",
       "      <td>183.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>0.100129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.698276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>28.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>-0.325301</td>\n",
       "      <td>0.106410</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>43.589744</td>\n",
       "      <td>23.675897</td>\n",
       "      <td>24.280000</td>\n",
       "      <td>340.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>0.099719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.244872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.087848</td>\n",
       "      <td>17.973684</td>\n",
       "      <td>32.210835</td>\n",
       "      <td>20.073808</td>\n",
       "      <td>31.710526</td>\n",
       "      <td>220.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>0.107390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.557833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>36.250000</td>\n",
       "      <td>108.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>0.124460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.524306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>13.709677</td>\n",
       "      <td>24.941176</td>\n",
       "      <td>15.460342</td>\n",
       "      <td>21.935484</td>\n",
       "      <td>106.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.135006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.390588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>34.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-0.227273</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>21.818182</td>\n",
       "      <td>38.888889</td>\n",
       "      <td>24.282828</td>\n",
       "      <td>35.848485</td>\n",
       "      <td>280.0</td>\n",
       "      <td>738.0</td>\n",
       "      <td>0.106952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.216667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0             34.0             5.0        0.743590            0.046931   \n",
       "1             58.0            31.0        0.303371            0.087771   \n",
       "2              NaN             NaN             NaN                 NaN   \n",
       "3             36.0            71.0       -0.327103            0.144205   \n",
       "4             21.0             8.0        0.448276            0.062500   \n",
       "..             ...             ...             ...                 ...   \n",
       "93            28.0            55.0       -0.325301            0.106410   \n",
       "94            26.0            34.0       -0.133333            0.087848   \n",
       "95             6.0             2.0        0.500000            0.027778   \n",
       "96            17.0             3.0        0.700000            0.047059   \n",
       "97            34.0            54.0       -0.227273            0.122222   \n",
       "\n",
       "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0             10.653846                    23.345367  13.599685   \n",
       "1             12.835443                    46.646943  23.792954   \n",
       "2                   NaN                          NaN        NaN   \n",
       "3             14.549020                    52.291105  26.736050   \n",
       "4             11.897436                    39.439655  20.534836   \n",
       "..                  ...                          ...        ...   \n",
       "93            15.600000                    43.589744  23.675897   \n",
       "94            17.973684                    32.210835  20.073808   \n",
       "95            24.000000                    37.500000  24.600000   \n",
       "96            13.709677                    24.941176  15.460342   \n",
       "97            21.818182                    38.888889  24.282828   \n",
       "\n",
       "    AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                          17.666667               194.0       878.0   \n",
       "1                          21.493671               473.0      1061.0   \n",
       "2                                NaN                 NaN         NaN   \n",
       "3                          23.745098               388.0       770.0   \n",
       "4                          19.589744               183.0       496.0   \n",
       "..                               ...                 ...         ...   \n",
       "93                         24.280000               340.0       809.0   \n",
       "94                         31.710526               220.0       709.0   \n",
       "95                         36.250000               108.0       296.0   \n",
       "96                         21.935484               106.0       450.0   \n",
       "97                         35.848485               280.0       738.0   \n",
       "\n",
       "    SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0            0.122911                0.0         5.356197  \n",
       "1            0.095202                0.0         6.458580  \n",
       "2                 NaN                NaN              NaN  \n",
       "3            0.093251                0.0         7.195418  \n",
       "4            0.100129                0.0         6.698276  \n",
       "..                ...                ...              ...  \n",
       "93           0.099719                0.0         6.244872  \n",
       "94           0.107390                0.0         5.557833  \n",
       "95           0.124460                0.0         6.524306  \n",
       "96           0.135006                0.0         5.390588  \n",
       "97           0.106952                0.0         6.216667  \n",
       "\n",
       "[98 rows x 13 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_excel(\"./Output Data Structure.xlsx\", \"Sheet1\")\n",
    "df0 = pd.DataFrame(analysis_score, columns = df1.columns[2:])\n",
    "empty_row = pd.Series([None] * len(df0.columns), index=df0.columns)\n",
    "empty_row\n",
    "df0.loc[2] = empty_row\n",
    "# for i in analysis_score:\n",
    "#     print(len(i))\n",
    "df0\n",
    "# error_404"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
